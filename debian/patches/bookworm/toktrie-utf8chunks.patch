error[E0599]: no method named `utf8_chunks` found for reference `&[u8]` in the current scope
   --> ../../third_party/rust/chromium_crates_io/vendor/toktrie-v0_7/src/toktree.rs:531:36
    |
531 |                 for chunk in bytes.utf8_chunks() {
    |                                    ^^^^^^^^^^^
    |

utf8_chunks() was added in rust 1.79:
https://doc.rust-lang.org/nightly/core/primitive.slice.html#method.utf8_chunks

We revert to an older chunk of toktree.rs code which is less robust, but
it'll have to do for now (until we upgrade bookworm's rustc-web).

--- a/third_party/rust/chromium_crates_io/vendor/toktrie-v0_7/src/toktree.rs
+++ b/third_party/rust/chromium_crates_io/vendor/toktrie-v0_7/src/toktree.rs
@@ -518,27 +518,25 @@ impl TokTrie {
 
     pub fn tokenize_with_greedy_fallback(
         &self,
-        bytes: &[u8],
-        str_tokenize: impl Fn(&str) -> Vec<TokenId>,
+        s: &[u8],
+        str_tokenize: impl FnOnce(&str) -> Vec<TokenId>,
     ) -> Vec<TokenId> {
-        match str::from_utf8(bytes) {
-            Ok(s) => {
-                // fast path
-                str_tokenize(s)
-            }
-            Err(_) => {
-                let mut res = vec![];
-                for chunk in bytes.utf8_chunks() {
-                    if !chunk.valid().is_empty() {
-                        res.extend(str_tokenize(chunk.valid()));
-                    }
-                    if !chunk.invalid().is_empty() {
-                        res.extend(self.greedy_tokenize(chunk.invalid()));
-                    }
-                }
-                res
-            }
+        let utf8_str = String::from_utf8_lossy(s);
+        // if the string ends with a replacement character, remove them
+        let to_tokenize = if utf8_str.ends_with('\u{FFFD}') {
+            utf8_str.trim_end_matches('\u{FFFD}')
+        } else {
+            &utf8_str
+        };
+        let mut r = str_tokenize(to_tokenize);
+        // if we didn't tokenize everything (because of the replacement character)
+        // we tokenize the suffix using greedy tokenizer that is happy with bytes
+        let last_tokenized = to_tokenize.len();
+        if last_tokenized < s.len() {
+            let mut added = self.greedy_tokenize(&s[last_tokenized..]);
+            r.append(&mut added);
         }
+        r
     }
 
     pub fn has_extensions(&self, bytes: &[u8]) -> bool {
